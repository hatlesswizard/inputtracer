// Package exploit provides exploitability assessment for taint analysis,
// inspired by ATLANTIS's approach to determining if vulnerabilities are actually exploitable.
//
// Key factors considered:
// - Sanitizer presence and effectiveness
// - Type coercion and casting
// - Condition guards and validation
// - Context of sink usage
// - Input vector characteristics
// - Attack surface analysis
package exploit

import (
	"regexp"
	"strings"
	"sync"
)

// ExploitabilityLevel represents the likelihood of exploitation
type ExploitabilityLevel int

const (
	LevelDefinitelyExploitable ExploitabilityLevel = 5
	LevelLikelyExploitable     ExploitabilityLevel = 4
	LevelPossiblyExploitable   ExploitabilityLevel = 3
	LevelUnlikelyExploitable   ExploitabilityLevel = 2
	LevelNotExploitable        ExploitabilityLevel = 1
	LevelUnknown               ExploitabilityLevel = 0
)

// VulnerabilityType represents the type of vulnerability
type VulnerabilityType string

const (
	VulnTypeSQLi       VulnerabilityType = "sqli"
	VulnTypeXSS        VulnerabilityType = "xss"
	VulnTypeCommandInj VulnerabilityType = "command_injection"
	VulnTypePathTraversal VulnerabilityType = "path_traversal"
	VulnTypeCodeExec   VulnerabilityType = "code_execution"
	VulnTypeSSRF       VulnerabilityType = "ssrf"
	VulnTypeDeserialization VulnerabilityType = "deserialization"
	VulnTypeXXE        VulnerabilityType = "xxe"
	VulnTypeOpenRedirect VulnerabilityType = "open_redirect"
	VulnTypeUnknown    VulnerabilityType = "unknown"
)

// Factor represents a factor affecting exploitability
type Factor struct {
	Name        string  `json:"name"`
	Type        string  `json:"type"`        // "positive" or "negative"
	Weight      float64 `json:"weight"`      // Impact on score
	Description string  `json:"description"`
	Evidence    string  `json:"evidence,omitempty"`
}

// Assessment represents an exploitability assessment
type Assessment struct {
	// Target information
	VulnType    VulnerabilityType   `json:"vuln_type"`
	SinkName    string              `json:"sink_name"`
	SourceName  string              `json:"source_name"`
	FilePath    string              `json:"file_path"`
	Line        int                 `json:"line"`

	// Assessment result
	Level       ExploitabilityLevel `json:"level"`
	Score       float64             `json:"score"`       // 0-100
	Confidence  float64             `json:"confidence"`  // 0-1

	// Factors affecting the assessment
	PositiveFactors []Factor        `json:"positive_factors"` // Make it more exploitable
	NegativeFactors []Factor        `json:"negative_factors"` // Make it less exploitable

	// Analysis details
	HasSanitizer     bool            `json:"has_sanitizer"`
	HasValidator     bool            `json:"has_validator"`
	HasTypeCoercion  bool            `json:"has_type_coercion"`
	HasAuthGuard     bool            `json:"has_auth_guard"`
	IsDirectFlow     bool            `json:"is_direct_flow"`    // Source directly to sink
	PathLength       int             `json:"path_length"`

	// Recommendations
	Recommendations []string         `json:"recommendations"`

	// Raw data
	TaintPath       []string         `json:"taint_path,omitempty"`
	Conditions      []string         `json:"conditions,omitempty"`
}

// Assessor evaluates exploitability of taint flows
type Assessor struct {
	mu sync.RWMutex

	// Sanitizer knowledge base
	effectiveSanitizers map[VulnerabilityType]map[string]float64 // vuln -> sanitizer -> effectiveness (0-1)
	partialSanitizers   map[VulnerabilityType]map[string]float64 // vuln -> sanitizer -> partial effectiveness

	// Dangerous patterns by vuln type
	dangerousPatterns map[VulnerabilityType][]*regexp.Regexp

	// Type coercion rules
	typeCoercions map[string]float64 // coercion -> impact on exploitability

	// Configuration
	defaultConfidence float64
}

// NewAssessor creates a new exploitability assessor
func NewAssessor() *Assessor {
	a := &Assessor{
		effectiveSanitizers: make(map[VulnerabilityType]map[string]float64),
		partialSanitizers:   make(map[VulnerabilityType]map[string]float64),
		dangerousPatterns:   make(map[VulnerabilityType][]*regexp.Regexp),
		typeCoercions:       make(map[string]float64),
		defaultConfidence:   0.7,
	}
	a.registerDefaults()
	return a
}

// registerDefaults sets up default knowledge base
func (a *Assessor) registerDefaults() {
	// SQL Injection sanitizers
	a.effectiveSanitizers[VulnTypeSQLi] = map[string]float64{
		"prepared_statement": 1.0,
		"parameterized":      1.0,
		"mysqli_real_escape_string": 0.9,
		"mysql_real_escape_string": 0.85,
		"pg_escape_string":   0.9,
		"addslashes":         0.3,  // Not effective for all cases
		"intval":             0.95, // Effective for numeric
		"floatval":           0.95,
		"(int)":              0.95,
		"(float)":            0.95,
	}

	// XSS sanitizers
	a.effectiveSanitizers[VulnTypeXSS] = map[string]float64{
		"htmlspecialchars":  0.95,
		"htmlentities":      0.95,
		"strip_tags":        0.6,  // Can be bypassed
		"html_entity_decode": 0.0, // Makes it worse!
		"urlencode":         0.7,
		"DOMPurify.sanitize": 0.95,
		"encodeURIComponent": 0.8,
	}

	// Command injection sanitizers
	a.effectiveSanitizers[VulnTypeCommandInj] = map[string]float64{
		"escapeshellarg":    0.95,
		"escapeshellcmd":    0.85,
		"shlex.quote":       0.95,  // Python
		"shellescape":       0.95,
	}

	// Path traversal sanitizers
	a.effectiveSanitizers[VulnTypePathTraversal] = map[string]float64{
		"basename":          0.9,
		"realpath":          0.85,
		"path.Clean":        0.85, // Go
		"filepath.Clean":    0.85, // Go
	}

	// Type coercions that affect exploitability
	a.typeCoercions = map[string]float64{
		"intval":    -0.8, // Reduces exploitability significantly
		"floatval":  -0.8,
		"(int)":     -0.8,
		"(float)":   -0.8,
		"(bool)":    -0.9,
		"(string)":  0.0,  // No effect
		"strval":    0.0,
		"parseInt":  -0.8,
		"parseFloat": -0.8,
		"int()":     -0.8, // Python
		"float()":   -0.8, // Python
		"str()":     0.0,
	}

	// Dangerous patterns by vuln type
	a.dangerousPatterns[VulnTypeSQLi] = []*regexp.Regexp{
		regexp.MustCompile(`(?i)SELECT\s+.*\s+FROM`),
		regexp.MustCompile(`(?i)INSERT\s+INTO`),
		regexp.MustCompile(`(?i)UPDATE\s+.*\s+SET`),
		regexp.MustCompile(`(?i)DELETE\s+FROM`),
		regexp.MustCompile(`(?i)UNION\s+SELECT`),
		regexp.MustCompile(`(?i)OR\s+['"]?\d+['"]?\s*=\s*['"]?\d+`),
	}

	a.dangerousPatterns[VulnTypeCommandInj] = []*regexp.Regexp{
		regexp.MustCompile(`;\s*\w+`),          // ; command
		regexp.MustCompile(`\|\s*\w+`),         // | pipe
		regexp.MustCompile(`\$\([^)]+\)`),      // $(command)
		regexp.MustCompile("`[^`]+`"),          // `command`
		regexp.MustCompile(`&&\s*\w+`),         // && command
		regexp.MustCompile(`\|\|\s*\w+`),       // || command
	}

	a.dangerousPatterns[VulnTypeXSS] = []*regexp.Regexp{
		regexp.MustCompile(`(?i)<script`),
		regexp.MustCompile(`(?i)javascript:`),
		regexp.MustCompile(`(?i)on\w+\s*=`),    // Event handlers
		regexp.MustCompile(`(?i)<img[^>]+src`),
		regexp.MustCompile(`(?i)<iframe`),
	}

	a.dangerousPatterns[VulnTypePathTraversal] = []*regexp.Regexp{
		regexp.MustCompile(`\.\.\/`),
		regexp.MustCompile(`\.\.\\`),
		regexp.MustCompile(`%2e%2e`),
		regexp.MustCompile(`%252e%252e`),
	}
}

// Assess evaluates the exploitability of a taint flow
func (a *Assessor) Assess(config *AssessmentConfig) *Assessment {
	assessment := &Assessment{
		VulnType:        config.VulnType,
		SinkName:        config.SinkName,
		SourceName:      config.SourceName,
		FilePath:        config.FilePath,
		Line:            config.Line,
		TaintPath:       config.TaintPath,
		Conditions:      config.Conditions,
		PathLength:      len(config.TaintPath),
		Confidence:      a.defaultConfidence,
		PositiveFactors: make([]Factor, 0),
		NegativeFactors: make([]Factor, 0),
		Recommendations: make([]string, 0),
	}

	// Start with base score
	score := 50.0

	// Analyze path characteristics
	a.analyzePathLength(assessment, &score)
	a.analyzeDirectFlow(config, assessment, &score)

	// Analyze sanitizers
	a.analyzeSanitizers(config, assessment, &score)

	// Analyze validators
	a.analyzeValidators(config, assessment, &score)

	// Analyze type coercions
	a.analyzeTypeCoercions(config, assessment, &score)

	// Analyze conditions/guards
	a.analyzeConditions(config, assessment, &score)

	// Analyze context
	a.analyzeContext(config, assessment, &score)

	// Calculate final score and level
	assessment.Score = clampScore(score)
	assessment.Level = a.scoreToLevel(assessment.Score)

	// Generate recommendations
	a.generateRecommendations(assessment)

	return assessment
}

// AssessmentConfig holds configuration for assessment
type AssessmentConfig struct {
	VulnType     VulnerabilityType
	SinkName     string
	SourceName   string
	FilePath     string
	Line         int
	TaintPath    []string  // Nodes in the taint path
	Conditions   []string  // Conditions guarding the path
	Sanitizers   []string  // Sanitizers in the path
	Validators   []string  // Validators in the path
	TypeCasts    []string  // Type casts/coercions in the path
	Context      string    // Additional context (e.g., code snippet)
}

func (a *Assessor) analyzePathLength(assessment *Assessment, score *float64) {
	// Direct source->sink is most dangerous
	if assessment.PathLength <= 2 {
		assessment.IsDirectFlow = true
		*score += 20
		assessment.PositiveFactors = append(assessment.PositiveFactors, Factor{
			Name:        "direct_flow",
			Type:        "positive",
			Weight:      20,
			Description: "Direct flow from source to sink with minimal processing",
		})
	} else if assessment.PathLength > 10 {
		// Long paths are harder to exploit
		*score -= 10
		assessment.NegativeFactors = append(assessment.NegativeFactors, Factor{
			Name:        "long_path",
			Type:        "negative",
			Weight:      10,
			Description: "Long taint path may indicate data transformations",
		})
	}
}

func (a *Assessor) analyzeDirectFlow(config *AssessmentConfig, assessment *Assessment, score *float64) {
	// Check if source is user-controlled
	userControlledSources := []string{
		"$_GET", "$_POST", "$_REQUEST", "$_COOKIE",
		"req.body", "req.query", "req.params",
		"request.GET", "request.POST", "request.data",
	}

	for _, src := range userControlledSources {
		if strings.Contains(config.SourceName, src) {
			*score += 15
			assessment.PositiveFactors = append(assessment.PositiveFactors, Factor{
				Name:        "user_controlled_source",
				Type:        "positive",
				Weight:      15,
				Description: "Source is directly user-controlled: " + config.SourceName,
			})
			break
		}
	}
}

func (a *Assessor) analyzeSanitizers(config *AssessmentConfig, assessment *Assessment, score *float64) {
	a.mu.RLock()
	effectiveSanitizers := a.effectiveSanitizers[config.VulnType]
	a.mu.RUnlock()

	for _, sanitizer := range config.Sanitizers {
		sanitizerLower := strings.ToLower(sanitizer)

		// Check effective sanitizers
		if effectiveness, ok := effectiveSanitizers[sanitizerLower]; ok {
			assessment.HasSanitizer = true

			if effectiveness >= 0.9 {
				*score -= 40
				assessment.NegativeFactors = append(assessment.NegativeFactors, Factor{
					Name:        "effective_sanitizer",
					Type:        "negative",
					Weight:      40,
					Description: "Effective sanitizer present: " + sanitizer,
					Evidence:    sanitizer,
				})
			} else if effectiveness >= 0.5 {
				*score -= 20
				assessment.NegativeFactors = append(assessment.NegativeFactors, Factor{
					Name:        "partial_sanitizer",
					Type:        "negative",
					Weight:      20,
					Description: "Partially effective sanitizer: " + sanitizer,
					Evidence:    sanitizer,
				})
			} else if effectiveness < 0.3 {
				// Weak sanitizer might give false sense of security
				*score += 5
				assessment.PositiveFactors = append(assessment.PositiveFactors, Factor{
					Name:        "weak_sanitizer",
					Type:        "positive",
					Weight:      5,
					Description: "Weak/bypassable sanitizer: " + sanitizer,
					Evidence:    sanitizer,
				})
			}
		}
	}
}

func (a *Assessor) analyzeValidators(config *AssessmentConfig, assessment *Assessment, score *float64) {
	for _, validator := range config.Validators {
		assessment.HasValidator = true

		validatorLower := strings.ToLower(validator)

		// Check for strong validators
		strongValidators := []string{"preg_match", "filter_var", "is_numeric", "ctype_"}
		for _, sv := range strongValidators {
			if strings.Contains(validatorLower, sv) {
				*score -= 25
				assessment.NegativeFactors = append(assessment.NegativeFactors, Factor{
					Name:        "strong_validator",
					Type:        "negative",
					Weight:      25,
					Description: "Strong input validation present: " + validator,
					Evidence:    validator,
				})
				break
			}
		}
	}
}

func (a *Assessor) analyzeTypeCoercions(config *AssessmentConfig, assessment *Assessment, score *float64) {
	a.mu.RLock()
	typeCoercions := a.typeCoercions
	a.mu.RUnlock()

	for _, cast := range config.TypeCasts {
		assessment.HasTypeCoercion = true

		if impact, ok := typeCoercions[strings.ToLower(cast)]; ok {
			if impact < 0 {
				*score += impact * 50 // Negative impact reduces exploitability
				assessment.NegativeFactors = append(assessment.NegativeFactors, Factor{
					Name:        "type_coercion",
					Type:        "negative",
					Weight:      -impact * 50,
					Description: "Type coercion limits attack surface: " + cast,
					Evidence:    cast,
				})
			}
		}
	}
}

func (a *Assessor) analyzeConditions(config *AssessmentConfig, assessment *Assessment, score *float64) {
	authPatterns := []string{
		"is_admin", "isAdmin", "logged_in", "isLoggedIn", "is_authenticated",
		"has_permission", "hasPermission", "check_auth", "requireAuth",
	}

	for _, cond := range config.Conditions {
		condLower := strings.ToLower(cond)

		// Check for auth guards
		for _, auth := range authPatterns {
			if strings.Contains(condLower, strings.ToLower(auth)) {
				assessment.HasAuthGuard = true
				*score -= 15
				assessment.NegativeFactors = append(assessment.NegativeFactors, Factor{
					Name:        "auth_guard",
					Type:        "negative",
					Weight:      15,
					Description: "Authentication/authorization check required",
					Evidence:    cond,
				})
				break
			}
		}
	}
}

func (a *Assessor) analyzeContext(config *AssessmentConfig, assessment *Assessment, score *float64) {
	a.mu.RLock()
	patterns := a.dangerousPatterns[config.VulnType]
	a.mu.RUnlock()

	// Check context for dangerous patterns
	for _, pattern := range patterns {
		if pattern.MatchString(config.Context) {
			*score += 10
			assessment.PositiveFactors = append(assessment.PositiveFactors, Factor{
				Name:        "dangerous_pattern",
				Type:        "positive",
				Weight:      10,
				Description: "Context contains dangerous pattern",
			})
			break
		}
	}

	// Check for concatenation (often indicates vulnerability)
	if strings.Contains(config.Context, " . ") || // PHP
		strings.Contains(config.Context, " + ") || // JS string concat
		strings.Contains(config.Context, "\" +") ||
		strings.Contains(config.Context, "' +") ||
		strings.Contains(config.Context, "f\"") || // Python f-string
		strings.Contains(config.Context, "f'") {
		*score += 15
		assessment.PositiveFactors = append(assessment.PositiveFactors, Factor{
			Name:        "string_concatenation",
			Type:        "positive",
			Weight:      15,
			Description: "String concatenation detected in sink context",
		})
	}
}

func (a *Assessor) scoreToLevel(score float64) ExploitabilityLevel {
	switch {
	case score >= 80:
		return LevelDefinitelyExploitable
	case score >= 60:
		return LevelLikelyExploitable
	case score >= 40:
		return LevelPossiblyExploitable
	case score >= 20:
		return LevelUnlikelyExploitable
	default:
		return LevelNotExploitable
	}
}

func (a *Assessor) generateRecommendations(assessment *Assessment) {
	if assessment.Level >= LevelPossiblyExploitable {
		switch assessment.VulnType {
		case VulnTypeSQLi:
			assessment.Recommendations = append(assessment.Recommendations,
				"Use parameterized queries or prepared statements",
				"Avoid string concatenation in SQL queries",
				"Implement input validation using allowlists")
		case VulnTypeXSS:
			assessment.Recommendations = append(assessment.Recommendations,
				"Use context-aware output encoding (htmlspecialchars, etc.)",
				"Implement Content Security Policy (CSP)",
				"Use a templating engine with auto-escaping")
		case VulnTypeCommandInj:
			assessment.Recommendations = append(assessment.Recommendations,
				"Avoid shell commands with user input",
				"Use escapeshellarg/escapeshellcmd if shell is necessary",
				"Use language-native APIs instead of shell commands")
		case VulnTypePathTraversal:
			assessment.Recommendations = append(assessment.Recommendations,
				"Validate file paths against allowlist",
				"Use basename() to strip directory traversal",
				"Implement chroot or jail for file operations")
		default:
			assessment.Recommendations = append(assessment.Recommendations,
				"Implement input validation",
				"Apply principle of least privilege",
				"Review security controls in the data flow path")
		}
	}

	if !assessment.HasSanitizer {
		assessment.Recommendations = append(assessment.Recommendations,
			"Add appropriate sanitization for this vulnerability type")
	}

	if !assessment.HasValidator {
		assessment.Recommendations = append(assessment.Recommendations,
			"Add input validation before processing user data")
	}
}

func clampScore(score float64) float64 {
	if score < 0 {
		return 0
	}
	if score > 100 {
		return 100
	}
	return score
}

// String returns a human-readable level name
func (l ExploitabilityLevel) String() string {
	switch l {
	case LevelDefinitelyExploitable:
		return "DEFINITELY_EXPLOITABLE"
	case LevelLikelyExploitable:
		return "LIKELY_EXPLOITABLE"
	case LevelPossiblyExploitable:
		return "POSSIBLY_EXPLOITABLE"
	case LevelUnlikelyExploitable:
		return "UNLIKELY_EXPLOITABLE"
	case LevelNotExploitable:
		return "NOT_EXPLOITABLE"
	default:
		return "UNKNOWN"
	}
}

// AddSanitizer adds a custom sanitizer with effectiveness for a vuln type
func (a *Assessor) AddSanitizer(vulnType VulnerabilityType, name string, effectiveness float64) {
	a.mu.Lock()
	defer a.mu.Unlock()

	if a.effectiveSanitizers[vulnType] == nil {
		a.effectiveSanitizers[vulnType] = make(map[string]float64)
	}
	a.effectiveSanitizers[vulnType][strings.ToLower(name)] = effectiveness
}

// AddDangerousPattern adds a dangerous pattern for a vuln type
func (a *Assessor) AddDangerousPattern(vulnType VulnerabilityType, pattern string) error {
	re, err := regexp.Compile(pattern)
	if err != nil {
		return err
	}

	a.mu.Lock()
	defer a.mu.Unlock()

	a.dangerousPatterns[vulnType] = append(a.dangerousPatterns[vulnType], re)
	return nil
}

// Stats returns assessor statistics
func (a *Assessor) Stats() map[string]int {
	a.mu.RLock()
	defer a.mu.RUnlock()

	totalSanitizers := 0
	for _, m := range a.effectiveSanitizers {
		totalSanitizers += len(m)
	}

	totalPatterns := 0
	for _, p := range a.dangerousPatterns {
		totalPatterns += len(p)
	}

	return map[string]int{
		"sanitizers":        totalSanitizers,
		"dangerous_patterns": totalPatterns,
		"type_coercions":    len(a.typeCoercions),
	}
}
